from dotenv import load_dotenv
load_dotenv()

from langchain_teddynote import logging
logging.langsmith('CH06-Chain')

from langchain.chains.conversation.base import ConversationChain
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain.prompts import PromptTemplate

template = """
ë‹¹ì‹ ì€ 10ë…„ì°¨ ì—‘ì…€ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ì•„ë˜ ëŒ€í™”ë‚´ìš©ì„ ë³´ê³  ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”

#ëŒ€í™”ë‚´ìš©
{context}
----
ğŸŒ±ê³ ê°: {question}
ğŸ¤–ì „ë¬¸ê°€:"""

prompt = PromptTemplate.from_template(template)

prompt.partial(chat_history="ì—‘ì…€ì—ì„œ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.")

class StreamingHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"{token}", end="", flush=True)

# LLM ëª¨ë¸ ì„¸íŒ…
stream_llm = ChatOpenAI(
    model="gpt-4-turbo-preview", streaming=True, callbacks=[StreamingHandler()]
)

memory = ConversationBufferMemory(
    memory_key="context",  # ëŒ€í™”ì˜ ë§¥ë½ì„ ì €ì¥í•˜ëŠ” í‚¤
    human_prefix="ğŸŒ±ê³ ê°",  # ì‚¬ëŒì´ ë§í•˜ëŠ” ë¶€ë¶„ì— ë¶™ëŠ” ì ‘ë‘ì‚¬
    ai_prefix="ğŸ¤–ì „ë¬¸ê°€",  # AIê°€ ë§í•˜ëŠ” ë¶€ë¶„ì— ë¶™ëŠ” ì ‘ë‘ì‚¬
)

# ëŒ€í™” ì„¸íŒ…
conversation = ConversationChain(
    llm=stream_llm,
    prompt=prompt,
    memory=memory,
    input_key="question",
    verbose=False,
)

print(f'# ì´ˆê¸° ë©”ëª¨ë¦¬ ê°’ : \n{conversation.memory.load_memory_variables({})} \n')

answer = conversation.predict(
    question="ì—‘ì…€ì—ì„œ VLOOKUP í•¨ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
)
print(answer)

print(f'\n # 1ë²ˆì§¸ ëŒ€í™” ì´í›„ ë©”ëª¨ë¦¬ ê°’ : \n{conversation.memory.load_memory_variables({})["context"]} \n')

answer = conversation.predict(
    question="ì‰¬ìš´ ì˜ˆì œë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‚˜ìš”?"
)
print(answer)

print(f'\n # 2ë²ˆì§¸ ëŒ€í™” ì´í›„ ë©”ëª¨ë¦¬ ê°’ : \n{conversation.memory.load_memory_variables({})["context"]} \n')